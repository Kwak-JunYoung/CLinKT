dataset_path: "./dataset"

checkpoint_dir: .ckpts
seed: 12405

sakt_config:
  embedding_size: 64
  num_blocks: 1
  num_attn_heads: 8
  dropout: 0.5

clsakt_config:
  embedding_size: 64
  num_blocks: 1
  num_attn_heads: 8
  dropout: 0.5

saint_config:
  embedding_size: 64
  num_blocks: 2
  num_attn_heads: 8
  dropout: 0.5

akt_config:
  ednet: 
    embedding_size: 64
    num_blocks: 2
    kq_same: True
    model_type: "akt"
    num_attn_heads: 8
    final_fc_dim: 512
    d_ff: 1024
    dropout: 0.2
    reg_l: 1
    separate_qr: False

  algebra05: 
    embedding_size: 64
    num_blocks: 2
    kq_same: True
    model_type: "akt"
    num_attn_heads: 8
    final_fc_dim: 512
    d_ff: 1024
    dropout: 0.05
    reg_l: 1
    separate_qr: False

  assistments09: 
    embedding_size: 64
    num_blocks: 2
    kq_same: True
    model_type: "akt"
    num_attn_heads: 8
    final_fc_dim: 512
    d_ff: 1024
    dropout: 0.2
    reg_l: 1
    separate_qr: False

  bridge06: 
    embedding_size: 64
    num_blocks: 2
    kq_same: True
    model_type: "akt"
    num_attn_heads: 8
    final_fc_dim: 512
    d_ff: 1024
    dropout: 0.2
    reg_l: 10
    separate_qr: False

  slepemapy: 
    embedding_size: 64
    num_blocks: 2
    kq_same: True
    model_type: "akt"
    num_attn_heads: 8
    final_fc_dim: 512
    d_ff: 1024
    dropout: 0.2
    reg_l: 10
    separate_qr: False

train_config:
  l2: 0.0
  log_wandb_fold: True
  sequence_option: "recent" # early or recent
  seq_len: 100
  batch_size: 512
  eval_batch_size: 512
  num_epochs: 300
  print_epochs: 1
  max_grad_norm: 2.0
  learning_rate: 0.001
  optimizer: adam
  diff_order: "random"
  
  loss: BCE

  ## Model Save
  save_model: False
  save_epochs: 1
  save_model_name: "tmp"
  log_path: "logs"
